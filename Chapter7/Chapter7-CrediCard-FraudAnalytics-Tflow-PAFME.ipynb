{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c94547",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Analytics\n",
    "## Chapter 7\n",
    "### Predictive Analytics for the Modern Enterprise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e008bb6",
   "metadata": {},
   "source": [
    "This is jupyter notebook that can be used to follow along the code examples for Chapter 7 Credit Card Fraud Analysis. We will focus on tenorflow examples in this Notebook\n",
    "\n",
    "The notebook has been tested using the following pre-requisite:\n",
    "\n",
    "- Python V3.9.13 - https://www.python.org/\n",
    "- Anaconda Navigator V3 for Python 3.9 - https://www.anaconda.com/\n",
    "- Jupyter - V6.4.12 - https://jupyter.org/\n",
    "- Desktop computer - macOS Ventura V13.1\n",
    "\n",
    "Documentation referece for Scikit Learn: https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca80814",
   "metadata": {},
   "source": [
    "### Pre-requisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a9255",
   "metadata": {},
   "source": [
    "In order to start using tensorflow we will need to install the tensorflow 2.0 in python. Use the followinf command\n",
    "\n",
    "```bash \n",
    "pip3 install tensorflow\n",
    "```\n",
    "\n",
    "- Original Dataset can be found here: [Dataset 1 - Credit Card fraud data on Kaggle ](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "- Local copy of the dataset can be downloaded here: https://github.com/paforme/predictiveanalytics/blob/main/Chapter7/Datasets/creditcard.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Tensorflow imports\n",
    "import tensorflow as tflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#Visualization imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "#Utilities\n",
    "import os, tempfile, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the Credit Card Dataset\n",
    "#Change this to the location where you downloaded and unzipped the dataset\n",
    "url = \"./Datasets/creditcard.csv\" \n",
    "data_df = pd.read_csv(url) #load data in a dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Time - It is relevant in the context of the user or the terminal\n",
    "#however we don't know much about the columns and hence their \n",
    "#relationship with time is not relevant\n",
    "data_cc = data_df.copy()\n",
    "data_cc.drop(['Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this code if you want to explore the features. \n",
    "#The loop will be CPU heavy for the machine running the notebook.\n",
    "\n",
    "features = data_cc.iloc[:,0:28].columns #Get all the features\n",
    "\n",
    "for f in features: #Loop through the features and for each\n",
    "\n",
    "    graph = pd.DataFrame(\n",
    "        #Create a data frame with the feature and the corresponding label\n",
    "        {str(f): data_cc[f], \"Class\": data_cc['Class'], } \n",
    "    )\n",
    "    \n",
    "    #Plot the feature distribution\n",
    "    fig = px.histogram(\n",
    "        graph,\n",
    "        x=f,\n",
    "        title=\"Feature \" + str(f) + \" Distribution\",\n",
    "        color=\"Class\",\n",
    "        marginal=\"box\",\n",
    "        labels={\"0\": \"Legitimate\", \"1\": \"Fraudulent\"},\n",
    "    )\n",
    "\n",
    "    fig.update_traces(opacity=0.75)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(data_cc['Class'])\n",
    "features = data_cc.drop(['Class'], axis = 1)\n",
    "\n",
    "#Create Training, Validation and Test sets\n",
    "train_features,test_features,train_label,test_label = train_test_split(\n",
    "    features,labels,test_size=0.20, random_state=110)\n",
    "train_features,val_features,train_label,val_label = train_test_split(\n",
    "    train_features,train_label,test_size=0.20, random_state=110)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization layer from tensorlfow used later on\n",
    "#Scale the features with Mean 0 and Standard deviation as 1 - so they can be interpreted on the same scale.\n",
    "#scaler = StandardScaler()\n",
    "#train_features = scaler.fit_transform(train_features) #Fit the scaler on training data\n",
    "#val_features = scaler.transform(val_features)\n",
    "#test_features = scaler.transform(test_features)\n",
    "\n",
    "#Clip features to remove outliers\n",
    "#train_features = np.clip(train_features, -5, 5)\n",
    "#val_features = np.clip(val_features, -5, 5)\n",
    "#test_features = np.clip(test_features, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca32528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the disbalance in the data\n",
    "fraud = len(data_cc[data_cc['Class']==1].index)\n",
    "legit = len(data_cc[data_cc['Class']==0].index)\n",
    "total = len(data_cc)\n",
    "\n",
    "print(\"Total fraudulent transactions: \", str(fraud))\n",
    "print(\"Total legitimate transactions: \", str(legit))\n",
    "print(\"Total transactions: \", str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8944ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate initial bias to improve training speed\n",
    "initial_bias = np.log([fraud/legit])\n",
    "\n",
    "#Define bias as a keras constant\n",
    "bias = tflow.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "#Define how we want to measure the performance of the model\n",
    "MON = [\n",
    "      keras.metrics.AUC(name='prc', curve='PR'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a056d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a normalizer\n",
    "#A preprocessing layer which normalizes continuous features\n",
    "normalizer = tflow.keras.layers.Normalization(axis=1) \n",
    "normalizer.adapt(np.array(train_features))\n",
    "\n",
    "#Build the model\n",
    "cc_model = tflow.keras.Sequential()\n",
    "cc_model.add(normalizer) #Add a pre-processing layer \n",
    "cc_model.add(layers.Dense(\n",
    "    16, activation='relu',input_shape=(train_features.shape[-1],))) \n",
    "#cc_model.add(layers.Dense(16, activation='relu',input_shape=(train_features.shape[-1],))) #Uncomment this for Multiple Hidden Layer NN\n",
    "cc_model.add(layers.Dropout(0.5))\n",
    "cc_model.add(keras.layers.Dense(1, activation='sigmoid',bias_initializer=bias)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69010e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Compile the model\n",
    "cc_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=MON\n",
    ")\n",
    "\n",
    "# Setting up parameters\n",
    "CYCLES = 100\n",
    "BATCH = 2048\n",
    "\n",
    "# Defining Early Stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_prc',  # Monitoring validation precision-recall curve\n",
    "    verbose=1,  # Verbosity level\n",
    "    patience=10,  # Number of epochs with no improvement \n",
    "    mode='max',  # Monitoring mode, maximizing precision-recall curve\n",
    "    restore_best_weights=True  # Restoring the best model weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at what the model looks like\n",
    "cc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model's initial weights\n",
    "init_weights = os.path.join(tempfile.mkdtemp(), 'iw')\n",
    "cc_model.save_weights(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c69f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_model.load_weights(init_weights) #Use this on a second run \n",
    "\n",
    "#Initialize tensorboard callback\n",
    "log_dir = \"logs/pafme/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tflow.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#Fit the model on the training data\n",
    "history = cc_model.fit(\n",
    "    train_features,\n",
    "    train_label,\n",
    "    batch_size=BATCH,\n",
    "    epochs=CYCLES,\n",
    "    callbacks=[early_stopping, tensorboard_callback],\n",
    "    validation_data=(val_features, val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/pafme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e60dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions for test dataset\n",
    "test_predictions_baseline = cc_model.predict(test_features, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b86792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a confusion matrix and test summary\n",
    "#def plot_matrix(actual, predictions, threshold=0.5):\n",
    "#    matrix = confusion_matrix(actual, predictions > threshold)\n",
    "#    plt.figure(figsize=(4,4))\n",
    "#    myplot = sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "\n",
    "#    plt.title('Actual/Prediction @{:.2f}'.format(threshold))\n",
    "#    plt.ylabel('Actual')\n",
    "#    plt.xlabel('Prediction')\n",
    "    \n",
    "#    print('\\033[1m' + 'Fraud summary (val = 1): ' '\\033[0m')\n",
    "#    print('- Total: ', np.sum(matrix[1])),\n",
    "#    print('- Detected: ', matrix[1][1]),\n",
    "#    print('- Missed: ', matrix[1][0]),\n",
    "    \n",
    "#    print('\\033[1m' + '\\nLegit summary (val = 0): ' '\\033[0m')\n",
    "#    print('- Total: ', np.sum(matrix[0])),\n",
    "#    print('- Detected: ', matrix[0][0]),\n",
    "#    print('- Missed: ', matrix[0][1])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4defe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_matrix(test_label, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc18a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(actual, predictions, threshold=0.5):\n",
    "    matrix = confusion_matrix(actual, predictions > threshold)\n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\", \n",
    "                xticklabels=['Legitimate', 'Fraud'], \n",
    "                yticklabels=['Legitimate', 'Fraud'])\n",
    "    plt.title('CC Fraud Transactions (Threshold: {:.2f})'.format(threshold))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    fraud_total, fraud_detected, fraud_missed = matrix[1].sum(), matrix[1][1], matrix[1][0]\n",
    "    legit_total, legit_detected, legit_missed = matrix[0].sum(), matrix[0][0], matrix[0][1]\n",
    "\n",
    "    print('\\033[1m' + 'Fraud summary (actual = 1): ' '\\033[0m')\n",
    "    print('- Total: ', fraud_total)\n",
    "    print('- Detected: ', fraud_detected)\n",
    "    print('- Missed: ', fraud_missed)\n",
    "\n",
    "    print('\\033[1m' + '\\nLegitimate summary (actual = 0): ' '\\033[0m')\n",
    "    print('- Total: ', legit_total)\n",
    "    print('- Detected: ', legit_detected)\n",
    "    print('- Missed: ', legit_missed)\n",
    "\n",
    "plot_matrix(test_label, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d6406",
   "metadata": {},
   "source": [
    "### Introducing Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight the classes to handle imbalance of data\n",
    "#class-weight =n_samples_total / (n_classes * n_samples_class)\n",
    "\n",
    "legit_weight = total / (legit * 2)\n",
    "fraud_weight = total / (fraud * 2)\n",
    "\n",
    "print('Weight (legit) class 0:', legit_weight)\n",
    "print('Weight (fraud) class 1:', fraud_weight)\n",
    "weight = {0: legit_weight, 1: fraud_weight}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 - Define initial bias\n",
    "bias = tflow.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "#Step 2 - Build the model\n",
    "weighted_model = tflow.keras.Sequential()\n",
    "weighted_model.add(normalizer) #Add a pre-processing layer\n",
    "weighted_model.add(layers.Dense(16, activation='relu',input_shape=(\n",
    "    train_features.shape[-1],)))\n",
    "weighted_model.add(layers.Dense(16, activation='relu',input_shape=(\n",
    "    train_features.shape[-1],))) #Uncomment this for Multiple Hidden Layer NN\n",
    "weighted_model.add(layers.Dropout(0.5))\n",
    "weighted_model.add(keras.layers.Dense(1, activation='sigmoid',\n",
    "                                      bias_initializer=bias)) \n",
    "weighted_model.load_weights(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1829f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 3 - Setup Tensorflow\n",
    "#Define Tensorboard log directory\n",
    "log_dir = \"logs/pafme/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "tensorboard_callback = tflow.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#Step 4 - Compile the model\n",
    "weighted_model.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=MON)\n",
    "\n",
    "weighted_model.load_weights(init_weights)\n",
    "\n",
    "#Step - 5 Fit the model\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_label,\n",
    "    batch_size=BATCH,\n",
    "    epochs=CYCLES,\n",
    "    callbacks=[early_stopping, tensorboard_callback],\n",
    "    validation_data=(val_features, val_label),\n",
    "    class_weight=weight) #Class weights are passed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/pafme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on test dataset\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c878076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot test prediction\n",
    "plot_matrix(test_label, test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f8af5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
